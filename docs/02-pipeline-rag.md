# Pipeline RAG do AMLDO

## üìã Sum√°rio

- [Introdu√ß√£o ao RAG](#introdu√ß√£o-ao-rag)
- [Pipeline v1 - B√°sico](#pipeline-v1---b√°sico)
- [Pipeline v2 - Aprimorado](#pipeline-v2---aprimorado)
- [Embeddings e Similaridade](#embeddings-e-similaridade)
- [Estrat√©gias de Busca](#estrat√©gias-de-busca)
- [Constru√ß√£o de Prompts](#constru√ß√£o-de-prompts)
- [Otimiza√ß√µes e Tuning](#otimiza√ß√µes-e-tuning)

## Introdu√ß√£o ao RAG

### O que √© RAG?

**RAG (Retrieval-Augmented Generation)** √© um padr√£o arquitetural que combina:

1. **Retrieval (Recupera√ß√£o)**: Busca de informa√ß√µes relevantes em uma base de conhecimento
2. **Augmented (Aumentada)**: Enriquecimento do contexto do modelo
3. **Generation (Gera√ß√£o)**: Cria√ß√£o de resposta usando LLM

### Por que RAG?

| Problema | Solu√ß√£o RAG |
|----------|-------------|
| LLMs t√™m conhecimento limitado ao treinamento | Busca informa√ß√µes atualizadas em documentos |
| LLMs "alucinam" informa√ß√µes inexistentes | Respostas fundamentadas em documentos reais |
| Dados sens√≠veis n√£o podem ir para OpenAI/Google | Conhecimento fica local, s√≥ query vai para API |
| Contexto fixo do modelo (4k-128k tokens) | Busca apenas trechos relevantes (~12 chunks) |

### Arquitetura RAG do AMLDO

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Pergunta   ‚îÇ "Qual o limite de dispensa?"
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  1. INDEXA√á√ÉO (Offline)            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ PDFs ‚Üí Chunks ‚Üí Embeddings   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ         ‚Üí FAISS Index        ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  2. RETRIEVAL (Online)             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Query Embedding ‚Üí FAISS      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ         ‚Üí Top-K Docs         ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  3. AUGMENTATION (v2 only)         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Sort Hierarchy ‚Üí Inject      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  Intros ‚Üí Structure XML      ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  4. GENERATION                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Prompt + Context ‚Üí Gemini    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ         ‚Üí Resposta           ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Resposta   ‚îÇ "Segundo Art. 75, Lei 14.133..."
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Pipeline v1 - B√°sico

### Vis√£o Geral

Pipeline direto e eficiente, sem p√≥s-processamento do contexto.

**Arquivo:** `rag_v1/tools.py`

### Etapas Detalhadas

#### 1. Inicializa√ß√£o (Carga do Sistema)

```python
# Carregamento do modelo de embeddings
modelo_embedding = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    encode_kwargs={"normalize_embeddings": True}
)

# Carregamento do √≠ndice FAISS (pr√©-constru√≠do)
vector_db = FAISS.load_local(
    "data/vector_db/v1_faiss_vector_db",
    embeddings=modelo_embedding,
    allow_dangerous_deserialization=True  # ‚ö†Ô∏è Somente com dados confi√°veis
)

# Inicializa√ß√£o do LLM
llm = init_chat_model("gemini-2.5-flash", model_provider="google_genai")
```

**Tempo:** ~5-10 segundos (primeira vez)
**Mem√≥ria:** ~500MB (modelo de embeddings + √≠ndice FAISS)

#### 2. Cria√ß√£o do Retriever

```python
def _get_retriever(vector_db=vector_db, search_type: str = "mmr", k: int = 12):
    return vector_db.as_retriever(
        search_type=search_type,  # "mmr" ou "similarity"
        search_kwargs={"k": k}    # n√∫mero de documentos
    )
```

**Par√¢metros:**
- `search_type="mmr"`: Maximal Marginal Relevance (diversidade)
- `k=12`: Retorna 12 chunks mais relevantes

#### 3. Execu√ß√£o da Consulta

```python
def _rag_answer(question: str, search_type: str = "mmr", k: int = 12) -> str:
    retriever = _get_retriever(search_type=search_type, k=k)

    # Template do prompt
    prompt = ChatPromptTemplate.from_template(
        "Use APENAS o contexto para responder.\n\n"
        "Contexto:\n{context}\n\n"
        "Pergunta:\n{question}"
    )

    # Chain do LangChain
    rag_chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    resposta = rag_chain.invoke(question)
    return resposta
```

**Fluxo:**

1. **Query Embedding:**
   - `question` ‚Üí `modelo_embedding.embed_query()` ‚Üí `vector[384]`

2. **FAISS Search:**
   - Busca MMR com k=12
   - Retorna documentos ordenados por relev√¢ncia e diversidade

3. **Contexto Direto:**
   ```
   Contexto:
   [Chunk 1 - Lei 14.133, Art. 75]
   [Chunk 2 - Lei 14.133, Art. 76]
   [Chunk 3 - Decreto 10.024, Art. 10]
   ...
   [Chunk 12]

   Pergunta:
   Qual o limite de dispensa?
   ```

4. **Gera√ß√£o (Gemini):**
   - LLM processa prompt completo
   - Gera resposta baseada apenas no contexto fornecido

**Tempo de Resposta:** ~2-5 segundos

### Vantagens v1

‚úÖ **Simplicidade**: C√≥digo limpo e direto
‚úÖ **Performance**: R√°pido, sem overhead de processamento
‚úÖ **Confiabilidade**: Menos pontos de falha
‚úÖ **Debug f√°cil**: Contexto simples de inspecionar

### Limita√ß√µes v1

‚ùå **Contexto desorganizado**: Chunks podem estar fora de ordem legal
‚ùå **Sem intros**: Perde contexto de t√≠tulos/cap√≠tulos
‚ùå **Redund√¢ncia**: Pode incluir `artigo_0.txt` na busca (duplica√ß√£o)

## Pipeline v2 - Aprimorado

### Vis√£o Geral

Pipeline com p√≥s-processamento sofisticado para organizar o contexto hierarquicamente.

**Arquivo:** `rag_v2/tools.py`

### Etapas Detalhadas

#### 1. Inicializa√ß√£o (Adicional)

```python
# Carrega CSV com artigos introdut√≥rios
df_art_0 = pd.read_csv('data/processed/v1_artigos_0.csv')
```

**Estrutura de `v1_artigos_0.csv`:**

| lei | titulo | capitulo | texto |
|-----|--------|----------|-------|
| L14133 | TITULO_0 | CAPITULO_0 | Lei n¬∫ 14.133, de 1¬∫ de abril de 2021... |
| L14133 | TITULO_II | CAPITULO_0 | T√çTULO II - DOS CONTRATOS ADMINISTRATIVOS... |
| L14133 | TITULO_II | CAPITULO_III | CAP√çTULO III - DAS GARANTIAS... |

#### 2. Retriever com Filtro

```python
def _get_retriever(vector_db=vector_db, search_type: str = SEARCH_TYPE, k: int = K):
    return vector_db.as_retriever(
        search_type=search_type,
        search_kwargs={
            "k": k,
            "filter": {
                "artigo": { "$nin": ['artigo_0.txt'] }  # Exclui artigo_0
            }
        }
    )
```

**Por que filtrar `artigo_0.txt`?**
- Esses arquivos cont√™m intros de cap√≠tulos/t√≠tulos
- Ser√£o inseridos manualmente nas posi√ß√µes corretas
- Evita duplica√ß√£o e desorganiza√ß√£o

#### 3. Recupera√ß√£o e Organiza√ß√£o

```python
def _rag_answer(question: str, search_type: str = SEARCH_TYPE, k: int = K) -> str:
    retriever = _get_retriever(search_type=search_type, k=k)

    # Invoca retriever
    contexto = retriever.invoke(question)

    # Converte para DataFrame
    linhas = []
    for doc in contexto:
        linhas.append({
            "texto": doc.page_content,
            **doc.metadata  # lei, titulo, capitulo, artigo, chunk_idx
        })

    # Ordena√ß√£o hier√°rquica
    df_resultados = pd.DataFrame(linhas).sort_values(
        ['lei', 'titulo', 'capitulo', 'artigo', 'chunk_idx']
    ).reset_index(drop=True)

    # ...continua
```

**Exemplo de Ordena√ß√£o:**

| lei | titulo | capitulo | artigo | chunk_idx | texto |
|-----|--------|----------|--------|-----------|-------|
| L14133 | TITULO_I | CAPITULO_I | artigo_1.txt | 0 | Art. 1¬∫ Esta Lei... |
| L14133 | TITULO_I | CAPITULO_I | artigo_1.txt | 1 | ...continua√ß√£o... |
| L14133 | TITULO_II | CAPITULO_III | artigo_15.txt | 0 | Art. 15. A contrata√ß√£o... |

#### 4. P√≥s-Processamento do Contexto

Esta √© a **inova√ß√£o principal do v2**.

```python
def get_pos_processed_context(df_resultados, df_art_0):
    df_cap = df_resultados[['lei', 'titulo', 'capitulo']].drop_duplicates()
    context = ''

    for law in df_cap['lei'].unique():
        context += f'\n\n<LEI {law}>\n'

        # Artigo_0 da lei (intro geral)
        art_0 = get_art_0(law, 'TITULO_0', 'CAPITULO_0', df_art_0)
        if art_0:
            context += f'{art_0}\n'

        df_law = df_cap[df_cap['lei']==law]

        for title in df_law['titulo'].unique():
            # Artigo_0 do t√≠tulo (intro do t√≠tulo)
            art_0 = get_art_0(law, title, 'CAPITULO_0', df_art_0)
            if art_0:
                context += f'{art_0}\n'

            if title != 'TITULO_0':
                context += f'<TITULO: {title}>\n'

            df_title = df_law[df_law['titulo']==title]

            for chapter in df_title['capitulo'].unique():
                # Artigo_0 do cap√≠tulo (intro do cap√≠tulo)
                art_0 = get_art_0(law, title, chapter, df_art_0)
                if art_0:
                    context += f'{art_0}\n'

                if chapter != 'CAPITULO_0':
                    context += f'<CAPITULO: {chapter}>\n'

                # Recupera chunks deste cap√≠tulo
                mask = (df_resultados['lei']==law) & \
                       (df_resultados['titulo']==title) & \
                       (df_resultados['capitulo']==chapter)
                df_chapter = df_resultados[mask]

                for artigo in df_chapter['artigo'].unique():
                    df_article = df_chapter[df_chapter['artigo']==artigo]
                    context += f'<ARTIGO: {artigo.replace(".txt", "")}>\n'

                    for _, row in df_article.iterrows():
                        context += f"{row['texto']}\n"

                    context += f'</ARTIGO: {artigo.replace(".txt", "")}>\n'

                if chapter != 'CAPITULO_0':
                    context += f'</CAPITULO: {chapter}>\n'

            if title != 'TITULO_0':
                context += f'</TITULO: {title}>\n'

        context += f'</LEI {law}>\n'

    return context
```

**Resultado (Exemplo):**

```xml
<LEI L14133>
Lei n¬∫ 14.133, de 1¬∫ de abril de 2021
Disp√µe sobre licita√ß√µes e contratos administrativos.

<TITULO: TITULO_II>
T√çTULO II - DOS CONTRATOS ADMINISTRATIVOS

<CAPITULO: CAPITULO_III>
CAP√çTULO III - DAS GARANTIAS

<ARTIGO: artigo_15>
Art. 15. A contrata√ß√£o poder√° ser precedida de garantia, a crit√©rio da Administra√ß√£o,
nas contrata√ß√µes de obras, servi√ßos e compras.

¬ß 1¬∫ Caber√° ao contratado optar por uma das seguintes modalidades de garantia:
I - cau√ß√£o em dinheiro;
II - seguro-garantia;
III - fian√ßa banc√°ria.
</ARTIGO: artigo_15>

</CAPITULO: CAPITULO_III>
</TITULO: TITULO_II>
</LEI L14133>
```

#### 5. Gera√ß√£o com Contexto Estruturado

```python
prompt = ChatPromptTemplate.from_template(
    "Use APENAS o contexto para responder.\n\n"
    f"<Contexto>:\n{context}\n</Contexto>\n\n"
    "<Pergunta>:\n{question}</Pergunta>\n\n"
)

rag_chain = (
    {"question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

resposta = rag_chain.invoke(question)
return resposta
```

### Vantagens v2

‚úÖ **Contexto organizado**: Hierarquia legal preservada
‚úÖ **Intros inclu√≠das**: LLM entende estrutura completa
‚úÖ **Refer√™ncias precisas**: F√°cil citar Lei, T√≠tulo, Cap√≠tulo, Artigo
‚úÖ **Melhor compreens√£o**: LLM v√™ relacionamento entre se√ß√µes
‚úÖ **Legibilidade**: Contexto mais f√°cil de auditar

### Limita√ß√µes v2

‚ùå **Complexidade**: Mais c√≥digo, mais pontos de falha
‚ùå **Performance**: ~1-2s mais lento (p√≥s-processamento)
‚ùå **Mem√≥ria**: Carrega CSV adicional (`df_art_0`)

## Embeddings e Similaridade

### Modelo de Embeddings

**Modelo:** `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`

**Caracter√≠sticas:**
- **Dimensionalidade:** 384 (menor que muitos modelos, mais eficiente)
- **Multil√≠ngue:** Treinado em 50+ idiomas, incluindo portugu√™s
- **Normalizado:** Embeddings com norma L2 = 1 (facilita c√°lculo de similaridade)
- **Dom√≠nio:** Par√°frase (entende reformula√ß√µes da mesma ideia)

### Como Funciona?

```python
# Exemplo simplificado
embedding_model = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    encode_kwargs={"normalize_embeddings": True}
)

# Query
query = "Qual o limite de dispensa?"
query_vector = embedding_model.embed_query(query)  # [384 floats]

# Documento
doc = "Art. 75. √â dispens√°vel a licita√ß√£o quando o valor for inferior a R$ 50.000,00"
doc_vector = embedding_model.embed_documents([doc])[0]  # [384 floats]

# Similaridade (cosseno)
similarity = np.dot(query_vector, doc_vector)  # ‚âà 0.82 (alta similaridade)
```

### C√°lculo de Similaridade

**Cosseno Similarity:**

$$
\text{similarity}(A, B) = \frac{A \cdot B}{||A|| \times ||B||}
$$

Como embeddings s√£o normalizados ($||A|| = ||B|| = 1$):

$$
\text{similarity}(A, B) = A \cdot B
$$

**Valores:**
- `1.0`: Id√™nticos semanticamente
- `0.8-0.9`: Muito similares
- `0.6-0.8`: Relacionados
- `< 0.6`: Pouco relacionados

## Estrat√©gias de Busca

### 1. Similarity Search (Similaridade Pura)

```python
retriever = vector_db.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 12}
)
```

**Como funciona:**
1. Calcula similaridade entre query e TODOS os chunks
2. Ordena por similaridade (maior ‚Üí menor)
3. Retorna top-12

**Vantagens:**
‚úÖ M√°xima relev√¢ncia individual
‚úÖ Mais r√°pido (sem p√≥s-processamento)

**Desvantagens:**
‚ùå Pode retornar chunks muito similares entre si (redund√¢ncia)
‚ùå Perde diversidade de aspectos

### 2. MMR - Maximal Marginal Relevance (Padr√£o)

```python
retriever = vector_db.as_retriever(
    search_type="mmr",
    search_kwargs={
        "k": 12,
        "fetch_k": 50,     # Busca 50 candidatos
        "lambda_mult": 0.5  # Balance relev√¢ncia vs diversidade
    }
)
```

**Como funciona:**

1. **Fase 1:** Busca `fetch_k` candidatos mais similares (ex: 50)
2. **Fase 2:** Itera `k` vezes (ex: 12):
   - Calcula score: `Œª * sim(query, doc) - (1-Œª) * max(sim(doc, docs_j√°_selecionados))`
   - Seleciona documento com maior score
   - Remove da lista de candidatos

**Par√¢metro `lambda_mult`:**
- `Œª = 1.0`: Apenas relev√¢ncia (= similarity search)
- `Œª = 0.5`: Balance (padr√£o)
- `Œª = 0.0`: Apenas diversidade (pode perder relev√¢ncia)

**Vantagens:**
‚úÖ Reduz redund√¢ncia
‚úÖ Cobre mais aspectos da lei
‚úÖ Melhor para perguntas amplas

**Desvantagens:**
‚ùå Pode perder chunks altamente relevantes mas similares
‚ùå Mais lento (~2x)

### Compara√ß√£o Pr√°tica

**Query:** "Quais s√£o os limites de dispensa?"

**Similarity Search (top-5):**
1. Art. 75, I - Dispensa at√© R$ 50.000,00 (obras)
2. Art. 75, I - Dispensa at√© R$ 50.000,00 (obras) [par√°grafo seguinte]
3. Art. 75, I - Dispensa at√© R$ 50.000,00 (obras) [continua√ß√£o]
4. Art. 75, II - Dispensa at√© R$ 10.000,00 (compras)
5. Art. 75, II - Dispensa at√© R$ 10.000,00 (compras) [par√°grafo]

**MMR (top-5):**
1. Art. 75, I - Dispensa at√© R$ 50.000,00 (obras)
2. Art. 75, II - Dispensa at√© R$ 10.000,00 (compras)
3. Art. 74 - Conceito de dispensa de licita√ß√£o
4. Art. 76 - Procedimento para dispensa
5. LCP 123, Art. 48 - Dispensa para MEI

‚Üí **MMR traz mais contexto variado!**

## Constru√ß√£o de Prompts

### Prompt v1 (Simples)

```python
prompt = ChatPromptTemplate.from_template(
    "Use APENAS o contexto para responder.\n\n"
    "Contexto:\n{context}\n\n"
    "Pergunta:\n{question}"
)
```

**Exemplo renderizado:**

```
Use APENAS o contexto para responder.

Contexto:
Art. 75. √â dispens√°vel a licita√ß√£o: I - para contrata√ß√£o que envolva valores...
[mais 11 chunks]

Pergunta:
Qual o limite de dispensa para obras?
```

### Prompt v2 (Estruturado)

```python
prompt = ChatPromptTemplate.from_template(
    "Use APENAS o contexto para responder.\n\n"
    f"<Contexto>:\n{context}\n</Contexto>\n\n"
    "<Pergunta>:\n{question}</Pergunta>\n\n"
)
```

**Exemplo renderizado:**

```
Use APENAS o contexto para responder.

<Contexto>:
<LEI L14133>
Lei n¬∫ 14.133, de 1¬∫ de abril de 2021

<TITULO: TITULO_IV>
T√çTULO IV - DAS DISPENSAS E DA INEXIGIBILIDADE DE LICITA√á√ÉO

<CAPITULO: CAPITULO_I>
CAP√çTULO I - DAS DISPENSAS

<ARTIGO: artigo_75>
Art. 75. √â dispens√°vel a licita√ß√£o:
I - para contrata√ß√£o que envolva valores inferiores a R$ 50.000,00 (cinquenta mil reais)...
</ARTIGO: artigo_75>

</CAPITULO: CAPITULO_I>
</TITULO: TITULO_IV>
</LEI L14133>
</Contexto>

<Pergunta>:
Qual o limite de dispensa para obras?
</Pergunta>
```

### Prompts System (Gemini)

O Gemini recebe automaticamente instru√ß√µes de sistema via Google ADK:

```
Voc√™ √© um agente RAG especializado em licita√ß√£o, dispensa por valor,
compliance, governan√ßa e normativos internos.

Regras:
1. Use a ferramenta consultar_base_rag para perguntas sobre legisla√ß√£o
2. Responda SOMENTE com informa√ß√µes do contexto fornecido
3. N√ÉO invente informa√ß√£o
4. Se n√£o encontrar resposta, diga claramente
5. Nunca mencione "estou usando ferramenta X"
```

## Otimiza√ß√µes e Tuning

### Par√¢metros Ajust√°veis

#### 1. N√∫mero de Documentos (k)

```python
k = 12  # Padr√£o atual
```

**Impacto:**
- ‚Üë `k`: Mais contexto, maior chance de resposta completa, mais tokens, mais caro
- ‚Üì `k`: Menos contexto, mais r√°pido, mais barato, risco de perder info

**Recomenda√ß√µes:**
- Perguntas simples: `k=5-8`
- Perguntas complexas: `k=12-20`
- Limite pr√°tico: `k=30` (contexto fica muito grande)

#### 2. Lambda (MMR)

```python
search_kwargs = {
    "k": 12,
    "lambda_mult": 0.5  # Ajust√°vel
}
```

**Experimentos:**
- `Œª=0.7`: Mais relev√¢ncia, menos diversidade
- `Œª=0.5`: Balance (padr√£o)
- `Œª=0.3`: Mais diversidade, menos relev√¢ncia individual

#### 3. Fetch K (MMR)

```python
search_kwargs = {
    "k": 12,
    "fetch_k": 50  # Candidatos iniciais
}
```

**Impacto:**
- ‚Üë `fetch_k`: Mais candidatos para escolher, mais diversidade, mais lento
- ‚Üì `fetch_k`: Menos op√ß√µes, mais r√°pido

**Recomenda√ß√£o:** `fetch_k = 3-5 √ó k`

### Tuning de Embeddings

**Chunk Size:**

```python
# get_vectorial_bank_v1.ipynb
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,      # Ajust√°vel
    chunk_overlap=50     # Ajust√°vel
)
```

**Trade-offs:**
- **Chunks grandes (1000+)**: Mais contexto por chunk, menos chunks necess√°rios, mas menos precis√£o
- **Chunks pequenos (200-500)**: Mais precis√£o, mais chunks recuperados, risco de fragmenta√ß√£o

**Atual:** 500 chars √© um bom balance para legisla√ß√£o

### M√©tricas de Avalia√ß√£o

#### 1. Relev√¢ncia

Medir se documentos recuperados s√£o relevantes:

```python
# Exemplo: conjunto de teste com perguntas e artigos esperados
queries_test = [
    {"query": "Qual limite dispensa obras?", "expected": ["artigo_75"]},
    # ...
]

for test in queries_test:
    docs = retriever.get_relevant_documents(test["query"])
    artigos_recuperados = [d.metadata["artigo"] for d in docs]
    hit = any(exp in artigos_recuperados for exp in test["expected"])
    # Calcular hit@k, MRR, etc.
```

#### 2. Resposta Correta

Avaliar qualidade da resposta final:

```python
# Usar LLM como juiz ou compara√ß√£o com respostas gold
def avaliar_resposta(pergunta, resposta_gerada, resposta_esperada):
    prompt = f"""
    Avalie se a resposta gerada est√° correta:

    Pergunta: {pergunta}
    Resposta Gerada: {resposta_gerada}
    Resposta Esperada: {resposta_esperada}

    Score (0-10):
    """
    # ...
```

#### 3. Lat√™ncia

```python
import time

start = time.time()
resposta = consultar_base_rag("Qual o limite?")
latency = time.time() - start

print(f"Lat√™ncia: {latency:.2f}s")
```

**Targets:**
- v1: < 3s
- v2: < 5s

---

**Pr√≥ximo:** [Estrutura de Dados](03-estrutura-dados.md)
